<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Wikisyno | Build a synonym table from Wikipedia</title>
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script src="javascripts/main.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

  </head>
  <body>

      <header>
        <h1>Wikisyno</h1>
        <p>Build a synonym table from Wikipedia</p>
      </header>

      <div id="banner">
        <span id="logo"></span>

        <a href="https://github.com/georgegg/wikiSyno" class="button fork"><strong>View On GitHub</strong></a>
        <div class="downloads">
          <span>Downloads:</span>
          <ul>
            <li><a href="https://github.com/georgegg/wikiSyno/zipball/master" class="button">ZIP</a></li>
            <li><a href="https://github.com/georgegg/wikiSyno/tarball/master" class="button">TAR</a></li>
          </ul>
        </div>
      </div><!-- end banner -->

    <div class="wrapper">
      <nav>
        <ul></ul>
      </nav>
      <section>
        <h1>wikiSyno</h1>

<h2>Build a synonym table from Wikipedia</h2>

<ol>
<li><p>Download a Wikipedia dump using the instructions at <a href="http://en.wikipedia.org/wiki/Wikipedia:Database_download">http://en.wikipedia.org/wiki/Wikipedia:Database_download</a> and store it in a MySQL database.</p></li>
<li><p>The Wikipedia is stored in a relational database, using the following schema. <a href="http://www.mediawiki.org/wiki/Manual:Database_layout">http://www.mediawiki.org/wiki/Manual:Database_layout</a>  We are only interested in two tables: "Page" and "Redirect" (see the lower right part of the schema). You can find the necessary files at <a href="http://dumps.wikimedia.org/enwiki/latest/">http://dumps.wikimedia.org/enwiki/latest/</a> which allow you to download each table individually.</p></li>
</ol><p>The documentation for the redirect table is at <a href="http://www.mediawiki.org/wiki/Manual:Redirect_table">http://www.mediawiki.org/wiki/Manual:Redirect_table</a>
The SQL file for creating the redirect table in a MySQL database is at <a href="http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-redirect.sql.gz">http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-redirect.sql.gz</a>
It is a 75Mb compressed file (~250Mb uncompressed)</p>

<p>The documentation for the page table is at <a href="http://www.mediawiki.org/wiki/Manual:Page_table">http://www.mediawiki.org/wiki/Manual:Page_table</a>
The SQL file for creating the page table in a MySQL database is at <a href="http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-page.sql.gz">http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-page.sql.gz</a>
It is a 860Mb compressed file (~2.5Gb uncompressed)</p>

<ol>
<li><p>Create a MySQL database with these two tables.</p></li>
<li><p>Using the redirect table, and the page table, we want a two column output that lists which page redirects to which. We need:
a. The page_id of the "from" page [from the "redirect" table]
b. The namespace and title of the "from" page [taken from the "page" table]
c. The namespace and title of the "to" page [taken from the "redirect" table]
d. The page_id of the "to" page (to which the page redirects) [taken from the "page" table]</p></li>
<li><p>Using the table created in Step 4, we want to have web service that:
a. takes as input a term
b. checks if the term exists in Wikipedia (as title either for a page or for a redirect)
b1. if it does not exist, returns an empty response, potentially with an error message
c. if the terms exists as a page/redirect, checks if it is a base page or a redirect page
c1. if it is a redirect page, find the base page that this term redirects to and then perform step 5 again until finding a base page
d. using the base page title, return as synonyms all the terms that redirect <em>to</em> this base page</p></li>
</ol><p>[Required component: The Steps 1-3 should be "almost automated". This means that there should be a script that we can run every month that fetches the new data from Wikipedia and updates the tables with the newest available data]</p>

<p>[Optional component: Check for disambiguation pages, using the "category" table in Wikipedia, and marking as disambiguation pages all pages within <a href="http://en.wikipedia.org/wiki/Category:Disambiguation_pages">http://en.wikipedia.org/wiki/Category:Disambiguation_pages</a> You will need to fetch a the extra necessary tables from <a href="http://dumps.wikimedia.org/enwiki/latest/">http://dumps.wikimedia.org/enwiki/latest/</a> ]</p>

<p>[Optional component: Restrict the entries in the page table to be only entries for which we know to be an "oDesk skill" and we have a Wikipedia page. We will provide you with the dictionary of skills that we use within oDesk] </p>

<h2>Expand the Wikipedia Synonyms service as follows:</h2>

<ul>
<li><p>Check for disambiguation pages, using the "category" table in Wikipedia, and marking as disambiguation pages all pages within <a href="http://en.wikipedia.org/wiki/Category:Disambiguation_pages">http://en.wikipedia.org/wiki/Category:Disambiguation_pages</a> You will need to fetch a the extra necessary tables from <a href="http://dumps.wikimedia.org/enwiki/latest/">http://dumps.wikimedia.org/enwiki/latest/</a></p></li>
<li><p>Import an "oDesk Skill" table and allow people to query and find synonyms for oDesk Skills (same service as the usual one, but restricted only to oDesk skills for querying). We will provide you with the dictionary of skills that we use within oDesk.</p></li>
<li><p>Examine how we can best use the StackExchange API, to get synonyms and related tags. For example, for Java, we get back the following synonyms: <a href="https://api.stackexchange.com/2.0/tags/%7Bjava%7D/synonyms?order=desc&amp;sort=creation&amp;site=stackoverflow">https://api.stackexchange.com/2.0/tags/%7Bjava%7D/synonyms?order=desc&amp;sort=creation&amp;site=stackoverflow</a> Documentation at <a href="https://api.stackexchange.com/docs/synonyms-by-tags">https://api.stackexchange.com/docs/synonyms-by-tags</a></p></li>
<li><p>Use Google Cloud SQL to store the tables and use Google App Engine to create the service.</p></li>
<li><p>Check in Google App Engine whether it is possible to automate the downloading of the gzipped SQL files from Wikipedia, and automate their execution every month or so.</p></li>
<li><p>Create a short public documentation of the service, using GitHub pages for hosting</p></li>
</ul><h1>Build</h1>

<h2><b> Steps 1-3:</b></h2>

<pre>
curl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-page.sql.gz -o page.sql.gz
gunzip page.sql.gz

curl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-category.sql.gz -o category.sql.gz
gunzip category.sql.gz

curl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pagelinks.sql.gz -o pagelinks.sql.gz
gunzip pagelinks.sql.gz

curl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-categorylinks.sql.gz -o categorylinks.sql.gz
gunzip categorylinks.sql.gz

curl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-redirect.sql.gz -o redirect.sql.gz
gunzip redirect.sql.gz

mysql --host=... --user=... --pass=... DB_NAME &lt; page.sql

mysql --host=... --user=... --pass=... DB_NAME &lt; pagelinks.sql

mysql --host=... --user=... --pass=... DB_NAME &lt; category.sql

mysql --host=... --user=... --pass=... DB_NAME &lt; categorylinks.sql

mysql --host=... --user=... --pass=... DB_NAME &lt; redirect.sql
</pre>

<p>These commands take approximately 2 hours to execute on Amazon RDS/MySQL (5 minutes for redirect.sql, two hours for page.sql), using the db.m2.4xlarge instance class. The tables are big, and you will need at least 10Gb free (preferably more, for peace of mind). Expect 6-7M entries in the redirect table and 27-30M entries for the page table.</p>

<h2>Step 4: After you have a db you create the table described in No 4:</h2>

<pre>
CREATE TABLE page_relation (
  sid int unsigned NOT NULL default 0,
  tid int unsigned NOT NULL default 0,
  snamespace int NOT NULL,
  tnamespace int NOT NULL,
  stitle varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  ttitle varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  PRIMARY KEY (sid, tid)
) 
DEFAULT CHARACTER SET = utf8
COLLATE = utf8_general_ci
</pre>

<p>and after that you can populate that</p>

<pre>INSERT IGNORE INTO page_relation
SELECT s.rd_from as sid, 
      t.page_id as tid, 
      p.page_namespace as snamespace,
      t.page_namespace as tnamespace, 
      p.page_title as stitle, 
      t.page_title as ttitle 
FROM redirect s 
JOIN page p ON (s.rd_from = p.page_id)
JOIN page t ON (s.rd_namespace = t.page_namespace AND s.rd_title = t.page_title)</pre>

This query will need approximately an hour to execute and the table will have 6M-7M entries. 

Since the queries will be executed mainly on that table, once you create the page_relation table, you will want to create indexes on all attributes

<pre>
CREATE INDEX ix_stitle ON DB_NAME.page_relation (stitle)

CREATE INDEX ix_ttitle ON DB_NAME.page_relation (ttitle)

CREATE INDEX ix_sid ON DB_NAME.page_relation (sid)
  
CREATE INDEX ix_tid ON DB_NAME.page_relation (tid)
</pre>

<p><b>Note!!!</b></p>

<p>We do not need to worry about chains of redirects. The query</p>

<pre>
SELECT
  sid,
    tid,
    snamespace,
    tnamespace,
    stitle,
    ttitle
FROM
    page_relation WHERE tid IN (SELECT sid FROM page_relation);
</pre>

<p>returns very few results, which are already fixed in the actual Wikipedia (so there seems to be an automatic process that fixes that part)</p>

<h2>Step 5: For this part of the project we created a mini platform with 2 actions (search and ajax).</h2>

<p>Search action implements a graphical representation of the search results whereas Ajax performs as service an return a json encoded data set.</p>

<p>A. Search</p>

<p>By visiting project/index.php you are asked to enter a term to search and if synonyms found they are return as an ordered list.</p>

<p>B. Ajax</p>

<p>By visiting project/index.php?action=ajax&amp;term=OUR TERM the search results are returned as a json encoded array </p>

<pre>
{synonyms:[],total:NUM}
</pre>

<p>The search is done by to queries (if needed) and 1 iteration of the first query results:</p>

<pre>
SELECT * FROM page_relation WHERE (stitle = 'TERM' OR ttitle = 'TERM') AND snamespace = 0 AND tnamespace = 0;

SELECT * FROM page_relation WHERE tid IN ARRAY_OF_BASE_PAGE_IDS_FROM_ITERATION;
</pre>

<h2>Step 6: Enhancement: We added a feature to search disambiguation pages so we add extra synonyms when searching for a keyword.</h2>

<p>We use 1 query to determine if a page is a disambiguous one and if it is an extra one to fetch those page links.</p>

<p><b>Determine if disambiguation:</b></p>

<pre>
SELECT categorylinks.cl_to 
FROM page 
JOIN categorylinks 
ON categorylinks.cl_from = page.page_id 
WHERE page.page_namespace = 0 AND page.page_title = 'OUR_PAGE_TITLE';
</pre>

<p><b>Fetch disambiguation page links:</b></p>

<pre>
SELECT * FROM pagelinks WHERE pl_namespace = 0 AND  pl_from = 'DISAMBIGUATION_PAGE_ID';
</pre>

<p>thus we have a change in our service (ajax)
we now get a json encoded array result like this:</p>

<pre>
{synonyms:[], disambiguation:[], total:NUM}
</pre>

<p><del><b>TODO: Enhance queries on 6 for faster results.</b></del></p>

<p><b>NEW!!! determine if disambiguation:</b></p>

<pre>
SELECT page.page_title as page, GROUP_CONCAT(categorylinks.cl_to) as categories 
FROM page 
JOIN categorylinks 
ON categorylinks.cl_from = page.page_id 
WHERE page.page_namespace = 0 AND page.page_title IN --ARRAY_OF_PAGES-- GROUP BY page.page_title;
</pre>

<h2>Step 7: Enhancement: Integrating with oDesk skills. </h2>

<p>(Issue #6 implementation)</p>

<p>Create a table with the list of oDesk skills:</p>

<pre>
CREATE TABLE /*_*/odesk_skills (
  skill varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL PRIMARY KEY,
  pretty_name varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  external_link varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  description TEXT CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  wikipedia_page_id int NULL DEFAULT NULL,
  freebase_machine_id varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL
)
DEFAULT CHARSET=utf8
COLLATE = utf8_general_ci;

CREATE INDEX /*i*/os_pretty_name ON /*_*/odesk_skills (pretty_name);
</pre>

<p>After that we search for skills in odesk skills table that match our synonyms and return them as array:</p>

<pre>
SELECT * FROM odesk_skills WHERE skill IN --ARRAY_OF_SYNONYMS_RETURNED_IN_STEP_5--
</pre>

<p>return in ajax (JSON) is now:</p>

<pre>
{synonyms:[], disambiguation:[], odesk:[], total:NUM}
</pre>

<h2>Step 8: Issues with capitalization and matching. (Issue #12)</h2>

<p>Using the query below we address a bit of the capitalization and matching issue:</p>

<pre>
SELECT * FROM page_relation 
WHERE (CONVERT(stitle USING latin1) COLLATE latin1_general_cs 'TERM' 
OR CONVERT(ttitle USING latin1) COLLATE latin1_general_cs = 'TERM') 
AND snamespace = 0 
AND tnamespace = 0;
</pre>

<p>We execute the query first and then we execute the case insensitive one if no results from the first one.
Though the query takes too long to be executed due to the on-the-fly conversion of the collation, so that should be a <b>temporary solution</b>.</p>

<p><b>TODO: The best solution would be in step 4 to create a double table where we use case sensitive collation to perform the query without conversion on-the-fly.</b></p>

<h1>Installation:</h1>

<h2>A. Requirements</h2>

<ol>
<li>WEB server</li>
<li>PHP</li>
<li>MySQL</li>
</ol><h2>B. DB setup</h2>

<ol>
<li>Create a DB</li>
<li>Run /Tables.sql to create schema and then /odesk_skills.sql to add odesk skills to the db.</li>
<li>Fetch and import latest data from mediawiki. <b>(heavy process!!!)</b>
</li>
</ol><pre>
curl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-page.sql.gz -o page.sql.gz
gunzip page.sql.gz

curl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-category.sql.gz -o category.sql.gz
gunzip category.sql.gz

curl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pagelinks.sql.gz -o pagelinks.sql.gz
gunzip pagelinks.sql.gz

curl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-categorylinks.sql.gz -o categorylinks.sql.gz
gunzip categorylinks.sql.gz

curl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-redirect.sql.gz -o redirect.sql.gz
gunzip redirect.sql.gz

mysql --host=... --user=... --pass=... DB_NAME &lt; page.sql

mysql --host=... --user=... --pass=... DB_NAME &lt; pagelinks.sql

mysql --host=... --user=... --pass=... DB_NAME &lt; category.sql

mysql --host=... --user=... --pass=... DB_NAME &lt; categorylinks.sql

mysql --host=... --user=... --pass=... DB_NAME &lt; redirect.sql
</pre>

<p>These commands take approximately 2 hours to execute on Amazon RDS/MySQL (5 minutes for redirect.sql, two hours for page.sql), using the db.m2.4xlarge instance class. The tables are big, and you will need at least 10Gb free (preferably more, for peace of mind). Expect 6-7M entries in the redirect table and 27-30M entries for the page table.</p>

<ol>
<li>Populate &amp; setup intermediate table (page_relation) by running the queries bellow:</li>
</ol><p><b>(heavy process!!!)</b></p>

<pre>
INSERT IGNORE INTO page_relation
SELECT s.rd_from as sid, 
      t.page_id as tid, 
      p.page_namespace as snamespace,
      t.page_namespace as tnamespace, 
      p.page_title as stitle, 
      t.page_title as ttitle 
FROM redirect s 
JOIN page p ON (s.rd_from = p.page_id)
JOIN page t ON (s.rd_namespace = t.page_namespace AND s.rd_title = t.page_title);
</pre>

<p>This query will need approximately an hour to execute and the table will have 6M-7M entries. </p>

<p>Since the queries will be executed mainly on that table, once you create the page_relation table, you will want to create indexes on all attributes</p>

<pre>
CREATE INDEX ix_stitle ON DB_NAME.page_relation (stitle);

CREATE INDEX ix_ttitle ON DB_NAME.page_relation (ttitle);

CREATE INDEX ix_sid ON DB_NAME.page_relation (sid);
  
CREATE INDEX ix_tid ON DB_NAME.page_relation (tid);
</pre>

<h2>B. Application setup</h2>

<p>Rename /config/config_dist.php to /config/config.php
Then edit the file and adjust parameters to your db connection.</p>

<h2>C. Running the Application</h2>

<p><b>GUI version:</b>
Navigate to your HOST_ROOT/wikisyno/index.php?action=search OR HOST_ROOT/wikisyno/index.php (default action is search).
There you can enter your query in the field and search for synonims.</p>

<p><b>API version:</b>
Navigate to your HOST_ROOT/wikisyno/index.php?action=ajax&amp;term=YOUR_QUERY
This will return a JSON encoded response with the results from search.
FORMAT:</p>

<pre>
{synonyms:[], disambiguation:[], odesk:[], total:NUM}
</pre>

<p>NOTE!!!: You can pass a parameter (for test and debug only) pretty_print=true in order to get a structured for of the JSON response for better view.
eg: HOST_ROOT/wikisyno/index.php?pretty_print=true&amp;action=ajax&amp;term=YOUR_QUERY</p>
      </section>
      <footer>
        <p>Project maintained by <a href="https://github.com/georgegg">georgegg</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="http://twitter.com/#!/michigangraham">mattgraham</a></small></p>
      </footer>
    </div>
    <!--[if !IE]><script>fixScale(document);</script><!--<![endif]-->
    
  </body>
</html>
