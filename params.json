{"note":"Don't delete this file! It's used internally to help with page regeneration.","body":"wikiSyno\r\n========\r\n\r\nBuild a synonym table from Wikipedia\r\n------------------------------------\r\n\r\n1. Download a Wikipedia dump using the instructions at http://en.wikipedia.org/wiki/Wikipedia:Database_download and store it in a MySQL database.\r\n\r\n2. The Wikipedia is stored in a relational database, using the following schema. http://www.mediawiki.org/wiki/Manual:Database_layout  We are only interested in two tables: \"Page\" and \"Redirect\" (see the lower right part of the schema). You can find the necessary files at http://dumps.wikimedia.org/enwiki/latest/ which allow you to download each table individually.\r\n\r\nThe documentation for the redirect table is at http://www.mediawiki.org/wiki/Manual:Redirect_table\r\nThe SQL file for creating the redirect table in a MySQL database is at http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-redirect.sql.gz\r\nIt is a 75Mb compressed file (~250Mb uncompressed)\r\n\r\nThe documentation for the page table is at http://www.mediawiki.org/wiki/Manual:Page_table\r\nThe SQL file for creating the page table in a MySQL database is at http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-page.sql.gz\r\nIt is a 860Mb compressed file (~2.5Gb uncompressed)\r\n\r\n3. Create a MySQL database with these two tables.\r\n\r\n4. Using the redirect table, and the page table, we want a two column output that lists which page redirects to which. We need:\r\na. The page_id of the \"from\" page [from the \"redirect\" table]\r\nb. The namespace and title of the \"from\" page [taken from the \"page\" table]\r\nc. The namespace and title of the \"to\" page [taken from the \"redirect\" table]\r\nd. The page_id of the \"to\" page (to which the page redirects) [taken from the \"page\" table]\r\n\r\n5. Using the table created in Step 4, we want to have web service that:\r\na. takes as input a term\r\nb. checks if the term exists in Wikipedia (as title either for a page or for a redirect)\r\nb1. if it does not exist, returns an empty response, potentially with an error message\r\nc. if the terms exists as a page/redirect, checks if it is a base page or a redirect page\r\nc1. if it is a redirect page, find the base page that this term redirects to and then perform step 5 again until finding a base page\r\nd. using the base page title, return as synonyms all the terms that redirect *to* this base page\r\n\r\n[Required component: The Steps 1-3 should be \"almost automated\". This means that there should be a script that we can run every month that fetches the new data from Wikipedia and updates the tables with the newest available data]\r\n\r\n[Optional component: Check for disambiguation pages, using the \"category\" table in Wikipedia, and marking as disambiguation pages all pages within http://en.wikipedia.org/wiki/Category:Disambiguation_pages You will need to fetch a the extra necessary tables from http://dumps.wikimedia.org/enwiki/latest/ ]\r\n\r\n[Optional component: Restrict the entries in the page table to be only entries for which we know to be an \"oDesk skill\" and we have a Wikipedia page. We will provide you with the dictionary of skills that we use within oDesk] \r\n\r\nExpand the Wikipedia Synonyms service as follows:\r\n-------------------------------------------------\r\n\r\n* Check for disambiguation pages, using the \"category\" table in Wikipedia, and marking as disambiguation pages all pages within http://en.wikipedia.org/wiki/Category:Disambiguation_pages You will need to fetch a the extra necessary tables from http://dumps.wikimedia.org/enwiki/latest/\r\n\r\n* Import an \"oDesk Skill\" table and allow people to query and find synonyms for oDesk Skills (same service as the usual one, but restricted only to oDesk skills for querying). We will provide you with the dictionary of skills that we use within oDesk.\r\n\r\n* Examine how we can best use the StackExchange API, to get synonyms and related tags. For example, for Java, we get back the following synonyms: https://api.stackexchange.com/2.0/tags/%7Bjava%7D/synonyms?order=desc&sort=creation&site=stackoverflow Documentation at https://api.stackexchange.com/docs/synonyms-by-tags\r\n\r\n* Use Google Cloud SQL to store the tables and use Google App Engine to create the service.\r\n\r\n* Check in Google App Engine whether it is possible to automate the downloading of the gzipped SQL files from Wikipedia, and automate their execution every month or so.\r\n\r\n* Create a short public documentation of the service, using GitHub pages for hosting\r\n\r\nBuild\r\n=====\r\n\r\n<b> Steps 1-3:</b>\r\n-------------------------------------------------------------------------------------------------------------------------------\r\n\r\n<pre>\r\ncurl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-page.sql.gz -o page.sql.gz\r\ngunzip page.sql.gz\r\n\r\ncurl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-category.sql.gz -o category.sql.gz\r\ngunzip category.sql.gz\r\n\r\ncurl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pagelinks.sql.gz -o pagelinks.sql.gz\r\ngunzip pagelinks.sql.gz\r\n\r\ncurl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-categorylinks.sql.gz -o categorylinks.sql.gz\r\ngunzip categorylinks.sql.gz\r\n\r\ncurl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-redirect.sql.gz -o redirect.sql.gz\r\ngunzip redirect.sql.gz\r\n\r\nmysql --host=... --user=... --pass=... DB_NAME &lt; page.sql\r\n\r\nmysql --host=... --user=... --pass=... DB_NAME &lt; pagelinks.sql\r\n\r\nmysql --host=... --user=... --pass=... DB_NAME &lt; category.sql\r\n\r\nmysql --host=... --user=... --pass=... DB_NAME &lt; categorylinks.sql\r\n\r\nmysql --host=... --user=... --pass=... DB_NAME &lt; redirect.sql\r\n</pre>\r\n\r\nThese commands take approximately 2 hours to execute on Amazon RDS/MySQL (5 minutes for redirect.sql, two hours for page.sql), using the db.m2.4xlarge instance class. The tables are big, and you will need at least 10Gb free (preferably more, for peace of mind). Expect 6-7M entries in the redirect table and 27-30M entries for the page table.\r\n\r\nStep 4: After you have a db you create the table described in No 4:\r\n-------------------------------------------------------------------------------------------------------------------------------\r\n\r\n<pre>\r\nCREATE TABLE page_relation (\r\n  sid int unsigned NOT NULL default 0,\r\n  tid int unsigned NOT NULL default 0,\r\n  snamespace int NOT NULL,\r\n  tnamespace int NOT NULL,\r\n  stitle varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,\r\n  ttitle varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,\r\n  PRIMARY KEY (sid, tid)\r\n) \r\nDEFAULT CHARACTER SET = utf8\r\nCOLLATE = utf8_general_ci\r\n</pre>\r\n\r\nand after that you can populate that\r\n\r\n<pre>INSERT IGNORE INTO page_relation\r\nSELECT s.rd_from as sid, \r\n      t.page_id as tid, \r\n      p.page_namespace as snamespace,\r\n      t.page_namespace as tnamespace, \r\n      p.page_title as stitle, \r\n      t.page_title as ttitle \r\nFROM redirect s \r\nJOIN page p ON (s.rd_from = p.page_id)\r\nJOIN page t ON (s.rd_namespace = t.page_namespace AND s.rd_title = t.page_title)</pre>\r\n\r\nThis query will need approximately an hour to execute and the table will have 6M-7M entries. \r\n\r\nSince the queries will be executed mainly on that table, once you create the page_relation table, you will want to create indexes on all attributes\r\n\r\n<pre>\r\nCREATE INDEX ix_stitle ON DB_NAME.page_relation (stitle)\r\n\r\nCREATE INDEX ix_ttitle ON DB_NAME.page_relation (ttitle)\r\n\r\nCREATE INDEX ix_sid ON DB_NAME.page_relation (sid)\r\n  \r\nCREATE INDEX ix_tid ON DB_NAME.page_relation (tid)\r\n</pre>\r\n\r\n<b>Note!!!</b>\r\n\r\nWe do not need to worry about chains of redirects. The query\r\n\r\n<pre>\r\nSELECT\r\n  sid,\r\n\ttid,\r\n\tsnamespace,\r\n\ttnamespace,\r\n\tstitle,\r\n\tttitle\r\nFROM\r\n\tpage_relation WHERE tid IN (SELECT sid FROM page_relation);\r\n</pre>\r\n\r\nreturns very few results, which are already fixed in the actual Wikipedia (so there seems to be an automatic process that fixes that part)\r\n\r\n\r\nStep 5: For this part of the project we created a mini platform with 2 actions (search and ajax).\r\n-------------------------------------------------------------------------------------------------------------------------------\r\n\r\nSearch action implements a graphical representation of the search results whereas Ajax performs as service an return a json encoded data set.\r\n\r\nA. Search\r\n\r\nBy visiting project/index.php you are asked to enter a term to search and if synonyms found they are return as an ordered list.\r\n\r\nB. Ajax\r\n\r\nBy visiting project/index.php?action=ajax&term=OUR TERM the search results are returned as a json encoded array \r\n<pre>\r\n{synonyms:[],total:NUM}\r\n</pre>\r\n\r\nThe search is done by to queries (if needed) and 1 iteration of the first query results:\r\n\r\n<pre>\r\nSELECT * FROM page_relation WHERE (stitle = 'TERM' OR ttitle = 'TERM') AND snamespace = 0 AND tnamespace = 0;\r\n\r\nSELECT * FROM page_relation WHERE tid IN ARRAY_OF_BASE_PAGE_IDS_FROM_ITERATION;\r\n</pre>\r\n\r\n\r\nStep 6: Enhancement: We added a feature to search disambiguation pages so we add extra synonyms when searching for a keyword.\r\n-------------------------------------------------------------------------------------------------------------------------------\r\nWe use 1 query to determine if a page is a disambiguous one and if it is an extra one to fetch those page links.\r\n\r\n<b>Determine if disambiguation:</b>\r\n\r\n<pre>\r\nSELECT categorylinks.cl_to \r\nFROM page \r\nJOIN categorylinks \r\nON categorylinks.cl_from = page.page_id \r\nWHERE page.page_namespace = 0 AND page.page_title = 'OUR_PAGE_TITLE';\r\n</pre>\r\n\r\n<b>Fetch disambiguation page links:</b>\r\n\r\n<pre>\r\nSELECT * FROM pagelinks WHERE pl_namespace = 0 AND  pl_from = 'DISAMBIGUATION_PAGE_ID';\r\n</pre>\r\n\r\nthus we have a change in our service (ajax)\r\nwe now get a json encoded array result like this:\r\n<pre>\r\n{synonyms:[], disambiguation:[], total:NUM}\r\n</pre>\r\n\r\n~~<b>TODO: Enhance queries on 6 for faster results.</b>~~\r\n\r\n<b>NEW!!! determine if disambiguation:</b>\r\n\r\n<pre>\r\nSELECT page.page_title as page, GROUP_CONCAT(categorylinks.cl_to) as categories \r\nFROM page \r\nJOIN categorylinks \r\nON categorylinks.cl_from = page.page_id \r\nWHERE page.page_namespace = 0 AND page.page_title IN --ARRAY_OF_PAGES-- GROUP BY page.page_title;\r\n</pre>\r\n\r\nStep 7: Enhancement: Integrating with oDesk skills. \r\n-------------------------------------------------------------------------------------------------------------------------------\r\n(Issue #6 implementation)\r\n\r\nCreate a table with the list of oDesk skills:\r\n<pre>\r\nCREATE TABLE /*_*/odesk_skills (\r\n  skill varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL PRIMARY KEY,\r\n  pretty_name varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,\r\n  external_link varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,\r\n  description TEXT CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,\r\n  wikipedia_page_id int NULL DEFAULT NULL,\r\n  freebase_machine_id varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL\r\n)\r\nDEFAULT CHARSET=utf8\r\nCOLLATE = utf8_general_ci;\r\n\r\nCREATE INDEX /*i*/os_pretty_name ON /*_*/odesk_skills (pretty_name);\r\n</pre>\r\n\r\nAfter that we search for skills in odesk skills table that match our synonyms and return them as array:\r\n\r\n<pre>\r\nSELECT * FROM odesk_skills WHERE skill IN --ARRAY_OF_SYNONYMS_RETURNED_IN_STEP_5--\r\n</pre>\r\n\r\nreturn in ajax (JSON) is now:\r\n\r\n<pre>\r\n{synonyms:[], disambiguation:[], odesk:[], total:NUM}\r\n</pre>\r\n\r\nStep 8: Issues with capitalization and matching. (Issue #12)\r\n-------------------------------------------------------------------------------------------------------------------------------\r\n\r\nUsing the query below we address a bit of the capitalization and matching issue:\r\n<pre>\r\nSELECT * FROM page_relation \r\nWHERE (CONVERT(stitle USING latin1) COLLATE latin1_general_cs 'TERM' \r\nOR CONVERT(ttitle USING latin1) COLLATE latin1_general_cs = 'TERM') \r\nAND snamespace = 0 \r\nAND tnamespace = 0;\r\n</pre>\r\nWe execute the query first and then we execute the case insensitive one if no results from the first one.\r\nThough the query takes too long to be executed due to the on-the-fly conversion of the collation, so that should be a <b>temporary solution</b>.\r\n\r\n<b>TODO: The best solution would be in step 4 to create a double table where we use case sensitive collation to perform the query without conversion on-the-fly.</b>\r\n\r\n\r\nInstallation:\r\n=============\r\n\r\nA. Requirements\r\n---------------\r\n1. WEB server\r\n2. PHP\r\n3. MySQL\r\n\r\nB. DB setup\r\n-----------\r\n\r\n1. Create a DB\r\n2. Run /Tables.sql to create schema and then /odesk_skills.sql to add odesk skills to the db.\r\n3. Fetch and import latest data from mediawiki. <b>(heavy process!!!)</b>\r\n\r\n<pre>\r\ncurl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-page.sql.gz -o page.sql.gz\r\ngunzip page.sql.gz\r\n\r\ncurl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-category.sql.gz -o category.sql.gz\r\ngunzip category.sql.gz\r\n\r\ncurl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pagelinks.sql.gz -o pagelinks.sql.gz\r\ngunzip pagelinks.sql.gz\r\n\r\ncurl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-categorylinks.sql.gz -o categorylinks.sql.gz\r\ngunzip categorylinks.sql.gz\r\n\r\ncurl http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-redirect.sql.gz -o redirect.sql.gz\r\ngunzip redirect.sql.gz\r\n\r\nmysql --host=... --user=... --pass=... DB_NAME &lt; page.sql\r\n\r\nmysql --host=... --user=... --pass=... DB_NAME &lt; pagelinks.sql\r\n\r\nmysql --host=... --user=... --pass=... DB_NAME &lt; category.sql\r\n\r\nmysql --host=... --user=... --pass=... DB_NAME &lt; categorylinks.sql\r\n\r\nmysql --host=... --user=... --pass=... DB_NAME &lt; redirect.sql\r\n</pre>\r\n\r\nThese commands take approximately 2 hours to execute on Amazon RDS/MySQL (5 minutes for redirect.sql, two hours for page.sql), using the db.m2.4xlarge instance class. The tables are big, and you will need at least 10Gb free (preferably more, for peace of mind). Expect 6-7M entries in the redirect table and 27-30M entries for the page table.\r\n\r\n4. Populate & setup intermediate table (page_relation) by running the queries bellow:\r\n\r\n<b>(heavy process!!!)</b>\r\n\r\n<pre>\r\nINSERT IGNORE INTO page_relation\r\nSELECT s.rd_from as sid, \r\n      t.page_id as tid, \r\n      p.page_namespace as snamespace,\r\n      t.page_namespace as tnamespace, \r\n      p.page_title as stitle, \r\n      t.page_title as ttitle \r\nFROM redirect s \r\nJOIN page p ON (s.rd_from = p.page_id)\r\nJOIN page t ON (s.rd_namespace = t.page_namespace AND s.rd_title = t.page_title);\r\n</pre>\r\n\r\nThis query will need approximately an hour to execute and the table will have 6M-7M entries. \r\n\r\nSince the queries will be executed mainly on that table, once you create the page_relation table, you will want to create indexes on all attributes\r\n\r\n<pre>\r\nCREATE INDEX ix_stitle ON DB_NAME.page_relation (stitle);\r\n\r\nCREATE INDEX ix_ttitle ON DB_NAME.page_relation (ttitle);\r\n\r\nCREATE INDEX ix_sid ON DB_NAME.page_relation (sid);\r\n  \r\nCREATE INDEX ix_tid ON DB_NAME.page_relation (tid);\r\n</pre>\r\n\r\nB. Setup application\r\n--------------------\r\n\r\nRename /config/config_dist.php to /config/config.php\r\nThen edit the file and adjust parameters to your db connection.\r\n\r\nC. Running the Application\r\n--------------------------\r\n\r\n<b>GUI version:</b>\r\nNavigate to your HOST_ROOT/wikisyno/index.php?action=search OR HOST_ROOT/wikisyno/index.php (default action is search).\r\nThere you can enter your query in the field and search for synonims.\r\n\r\n<b>API version:</b>\r\nNavigate to your HOST_ROOT/wikisyno/index.php?action=ajax&term=YOUR_QUERY\r\nThis will return a JSON encoded response with the results from search.\r\nFORMAT:\r\n<pre>\r\n{synonyms:[], disambiguation:[], odesk:[], total:NUM}\r\n</pre>\r\n\r\nNOTE!!!: You can pass a parameter (for test and debug only) pretty_print=true in order to get a structured for of the JSON response for better view.\r\neg: HOST_ROOT/wikisyno/index.php?pretty_print=true&action=ajax&term=YOUR_QUERY\r\n\r\n","name":"Wikisyno","google":"","tagline":"Build a synonym table from Wikipedia"}